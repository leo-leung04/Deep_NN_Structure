Top 20 layer types by task:

feature-extraction:
  Linear: 4627 (36.0%)
  Dropout: 1776 (13.8%)
  LayerNorm: 1390 (10.8%)
  GELUActivation: 456 (3.5%)
  NewGELUActivation: 208 (1.6%)
  T5LayerNorm: 161 (1.3%)
  Embedding: 145 (1.1%)
  Conv1D: 144 (1.1%)
  FunnelLayer: 104 (0.8%)
  FunnelRelMultiheadAttention: 104 (0.8%)
  FunnelPositionwiseFFN: 104 (0.8%)
  BertLayer: 88 (0.7%)
  BertOutput: 88 (0.7%)
  BertIntermediate: 88 (0.7%)
  BertAttention: 88 (0.7%)
  BertSelfOutput: 88 (0.7%)
  BertSdpaSelfAttention: 88 (0.7%)
  T5Block: 78 (0.6%)
  T5Attention: 78 (0.6%)
  T5LayerSelfAttention: 78 (0.6%)

image-text-to-text:
  Linear: 1357 (36.9%)
  LayerNorm: 510 (13.9%)
  Dropout: 466 (12.7%)
  GELUActivation: 187 (5.1%)
  BlipTextAttention: 106 (2.9%)
  BlipTextSelfAttention: 106 (2.9%)
  BlipTextSelfOutput: 106 (2.9%)
  BlipMLP: 65 (1.8%)
  BlipAttention: 65 (1.8%)
  BlipEncoderLayer: 65 (1.8%)
  BlipTextIntermediate: 53 (1.4%)
  BlipTextLayer: 53 (1.4%)
  BlipTextOutput: 53 (1.4%)
  Blip2MLP: 39 (1.1%)
  Blip2EncoderLayer: 39 (1.1%)
  Blip2Attention: 39 (1.1%)
  OPTSdpaAttention: 32 (0.9%)
  ReLU: 32 (0.9%)
  OPTDecoderLayer: 32 (0.9%)
  Blip2QFormerSelfOutput: 18 (0.5%)

audio-classification:
  Linear: 2103 (29.8%)
  Dropout: 1497 (21.2%)
  GELUActivation: 930 (13.2%)
  LayerNorm: 779 (11.0%)
  Conv1d: 209 (3.0%)
  WhisperSdpaAttention: 204 (2.9%)
  SiLU: 136 (1.9%)
  _WeightNorm: 128 (1.8%)
  ParametrizationList: 128 (1.8%)
  WhisperEncoderLayer: 68 (1.0%)
  WhisperDecoderLayer: 68 (1.0%)
  StableDropout: 48 (0.7%)
  ReLU: 48 (0.7%)
  Wav2Vec2LayerNormConvLayer: 40 (0.6%)
  Wav2Vec2ConformerFeedForward: 37 (0.5%)
  GLU: 36 (0.5%)
  MambaRMSNorm: 33 (0.5%)
  MambaMixer: 32 (0.5%)
  MambaBlock: 32 (0.5%)
  Wav2Vec2FeedForward: 24 (0.3%)

automatic-speech-recognition:
  Linear: 1680 (39.7%)
  Dropout: 635 (15.0%)
  LayerNorm: 573 (13.6%)
  GELUActivation: 487 (11.5%)
  WhisperSdpaAttention: 216 (5.1%)
  Conv1d: 78 (1.8%)
  WhisperEncoderLayer: 72 (1.7%)
  WhisperDecoderLayer: 72 (1.7%)
  _WeightNorm: 56 (1.3%)
  ParametrizationList: 56 (1.3%)
  SpeechT5Attention: 24 (0.6%)
  Speech2TextAttention: 24 (0.6%)
  Wav2Vec2LayerNormConvLayer: 20 (0.5%)
  ReLU: 18 (0.4%)
  SpeechT5FeedForward: 18 (0.4%)
  BartSdpaAttention: 18 (0.4%)
  ModuleList: 13 (0.3%)
  Wav2Vec2FeedForward: 12 (0.3%)
  Wav2Vec2NoLayerNormConvLayer: 12 (0.3%)
  SpeechT5EncoderLayer: 12 (0.3%)

image-feature-extraction:
  Linear: 278 (25.5%)
  Dropout: 152 (14.0%)
  Conv1D: 96 (8.8%)
  LayerNorm: 55 (5.1%)
  RMSNorm: 50 (4.6%)
  ImageGPTLayerNorm: 49 (4.5%)
  GELUActivation: 26 (2.4%)
  ImageGPTBlock: 24 (2.2%)
  AIMv2Block: 24 (2.2%)
  AIMv2SwiGLUFFN: 24 (2.2%)
  ImageGPTMLP: 24 (2.2%)
  QuickGELUActivation: 24 (2.2%)
  ImageGPTAttention: 24 (2.2%)
  AIMv2Attention: 24 (2.2%)
  Data2VecVisionDropPath: 22 (2.0%)
  ViTOutput: 14 (1.3%)
  ViTLayer: 14 (1.3%)
  ViTAttention: 14 (1.3%)
  ViTIntermediate: 14 (1.3%)
  ViTSelfOutput: 14 (1.3%)

image-segmentation:
  Linear: 2293 (25.1%)
  LayerNorm: 954 (10.5%)
  Dropout: 879 (9.6%)
  Conv2d: 647 (7.1%)
  GELUActivation: 290 (3.2%)
  BatchNorm2d: 274 (3.0%)
  ReLU: 260 (2.9%)
  REBNCONV: 112 (1.2%)
  SegformerMixFFN: 104 (1.1%)
  SegformerSelfOutput: 104 (1.1%)
  SegformerEfficientSelfAttention: 104 (1.1%)
  SegformerDWConv: 104 (1.1%)
  SegformerLayer: 104 (1.1%)
  SegformerAttention: 104 (1.1%)
  Sequential: 100 (1.1%)
  SegformerDropPath: 100 (1.1%)
  ModuleList: 74 (0.8%)
  LeakyReLU: 66 (0.7%)
  MobileViTV2ConvLayer: 64 (0.7%)
  Identity: 62 (0.7%)

fill-mask:
  Linear: 6642 (35.1%)
  LayerNorm: 2240 (11.8%)
  Dropout: 2190 (11.6%)
  GELUActivation: 1036 (5.5%)
  BertSdpaSelfAttention: 432 (2.3%)
  BertSelfOutput: 432 (2.3%)
  BertAttention: 432 (2.3%)
  BertIntermediate: 432 (2.3%)
  BertOutput: 432 (2.3%)
  BertLayer: 432 (2.3%)
  Embedding: 230 (1.2%)
  RobertaSdpaSelfAttention: 198 (1.0%)
  RobertaLayer: 198 (1.0%)
  RobertaOutput: 198 (1.0%)
  RobertaAttention: 198 (1.0%)
  RobertaSelfOutput: 198 (1.0%)
  RobertaIntermediate: 198 (1.0%)
  XLMRobertaAttention: 76 (0.4%)
  XLMRobertaIntermediate: 76 (0.4%)
  XLMRobertaOutput: 76 (0.4%)

